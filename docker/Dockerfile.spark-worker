# Dockerfile for Spark Worker with Lakehouse frameworks
FROM apache/spark:3.5.0-python3

USER root

# Install Python packages for lakehouse frameworks
RUN pip install --no-cache-dir \
    delta-spark==3.0.0 \
    pyiceberg==0.6.1 \
    python-dotenv==1.0.1 \
    structlog==24.1.0 \
    pandas==2.0.3 \
    pyarrow==13.0.0

# Download lakehouse JARs (same as master)
RUN cd /opt/spark/jars && \
    # Delta Lake
    curl -L -O https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.0.0/delta-spark_2.12-3.0.0.jar && \
    curl -L -O https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.jar && \
    # Iceberg
    curl -L -O https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.4.3/iceberg-spark-runtime-3.5_2.12-1.4.3.jar && \
    # Hudi
    curl -L -O https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.5-bundle_2.12/1.0.2/hudi-spark3.5-bundle_2.12-1.0.2.jar && \
    # Verify JARs were downloaded correctly
    ls -lh *.jar && \
    echo "âœ… All JARs downloaded successfully"

# Create directories
RUN mkdir -p /data /opt/spark/jobs

WORKDIR /opt/spark

# Copy entrypoint script
COPY docker/worker-entrypoint.sh /opt/spark/worker-entrypoint.sh
RUN chmod +x /opt/spark/worker-entrypoint.sh

# Set default values for worker resources
ENV SPARK_WORKER_CORES=5
ENV SPARK_WORKER_MEMORY=3g

EXPOSE 8081

ENTRYPOINT ["/opt/spark/worker-entrypoint.sh"]
