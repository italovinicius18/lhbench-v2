"""
Script de exemplo para executar o LHBench standalone
"""
import os
import sys

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from standalone import run_full_pipeline, config

def exemplo_basico():
    """Exemplo b√°sico - pipeline completo com SF=1"""
    print("üöÄ Executando LHBench com Scale Factor 1")
    
    # Configurar para teste r√°pido
    results = run_full_pipeline(
        scale_factor=1,
        benchmark_formats=["delta"],  # Apenas Delta para teste
        benchmark_iterations=1,       # 1 itera√ß√£o apenas
        force_regenerate_bronze=False
    )
    
    print(f"\nüìä Status final: {results['status']}")
    return results

def exemplo_completo():
    """Exemplo completo - todos os formatos"""
    print("üöÄ Executando LHBench completo")
    
    results = run_full_pipeline(
        scale_factor=10,
        benchmark_formats=["delta", "iceberg", "hudi"],
        benchmark_iterations=3,
        force_regenerate_bronze=False,
        force_recreate_silver=False
    )
    
    print(f"\nüìä Status final: {results['status']}")
    
    # Mostrar ranking se benchmark executado
    if results["status"] == "success" and "benchmark" in results["stages"]:
        benchmark_data = results["stages"]["benchmark"]
        if "comparison" in benchmark_data and "format_ranking" in benchmark_data["comparison"]:
            print("\nüèÜ Ranking de Performance:")
            for i, fmt in enumerate(benchmark_data["comparison"]["format_ranking"], 1):
                print(f"   {i}. {fmt['format'].upper()}: {fmt['overall_avg_time']:.2f}s")
    
    return results

def exemplo_bronze_apenas():
    """Exemplo - apenas gera√ß√£o bronze"""
    from standalone import generate_bronze_data
    
    print("ü•â Executando apenas gera√ß√£o Bronze")
    
    results = generate_bronze_data(
        scale_factor=1,
        force_regenerate=True
    )
    
    print(f"\nüìä Status: {results['status']}")
    if results['status'] == 'success':
        print(f"üìÅ Tabelas geradas: {results['tables_generated']}")
        print(f"‚è±Ô∏è  Tempo: {results['duration_seconds']:.2f}s")
    
    return results

def exemplo_silver_apenas():
    """Exemplo - apenas convers√£o silver"""
    from standalone import convert_to_silver_format
    
    print("ü•à Executando apenas convers√£o Silver (Delta)")
    
    results = convert_to_silver_format(
        format_name="delta",
        scale_factor=1,
        force_recreate=False
    )
    
    print(f"\nüìä Status: {results['status']}")
    if results['status'] == 'success':
        print(f"üìÅ Tabelas convertidas: {results['tables_successful']}/{results['tables_processed']}")
        print(f"‚è±Ô∏è  Tempo: {results['duration_seconds']:.2f}s")
    
    return results

def exemplo_benchmark_apenas():
    """Exemplo - apenas benchmark"""
    from standalone import benchmark_format
    
    print("ü•á Executando apenas Benchmark (Delta)")
    
    results = benchmark_format(
        format_name="delta",
        scale_factor=1,
        iterations=1,
        run_warmup=True,
        timeout_minutes=10
    )
    
    print(f"\nüìä Status: {results['status']}")
    if results['status'] == 'success':
        print(f"üìÅ Queries executadas: {results['successful_queries']}/{results['total_queries']}")
        print(f"‚è±Ô∏è  Tempo: {results['duration_seconds']:.2f}s")
    
    return results

def exemplo_configuracao_customizada():
    """Exemplo com configura√ß√£o customizada"""
    from standalone import LHBenchConfig, run_full_pipeline
    
    # Criar configura√ß√£o customizada
    custom_config = LHBenchConfig(
        scale_factor=1,
        minio_endpoint="http://localhost:9000",
        minio_access_key="minioadmin",
        minio_secret_key="minioadmin",
        benchmark_formats=["delta", "iceberg"],
        benchmark_iterations=2,
        spark_executor_memory="2g",
        spark_driver_memory="1g",
        results_path="./custom_results"
    )
    
    print("üîß Executando com configura√ß√£o customizada")
    print(f"   Scale Factor: {custom_config.scale_factor}")
    print(f"   Formatos: {custom_config.benchmark_formats}")
    print(f"   Spark Memory: {custom_config.spark_executor_memory}")
    
    # Aplicar configura√ß√£o (substituir global temporariamente)
    import standalone.config
    original_config = standalone.config.config
    standalone.config.config = custom_config
    
    try:
        results = run_full_pipeline()
        print(f"\nüìä Status final: {results['status']}")
        return results
    finally:
        # Restaurar configura√ß√£o original
        standalone.config.config = original_config

if __name__ == "__main__":
    print("LHBench Standalone - Exemplos de Uso")
    print("="*50)
    
    # Verificar argumentos
    if len(sys.argv) > 1:
        exemplo = sys.argv[1].lower()
        
        if exemplo == "basico":
            exemplo_basico()
        elif exemplo == "completo":
            exemplo_completo()
        elif exemplo == "bronze":
            exemplo_bronze_apenas()
        elif exemplo == "silver":
            exemplo_silver_apenas()
        elif exemplo == "benchmark":
            exemplo_benchmark_apenas()
        elif exemplo == "custom":
            exemplo_configuracao_customizada()
        else:
            print(f"‚ùå Exemplo '{exemplo}' n√£o reconhecido")
            print("Exemplos dispon√≠veis: basico, completo, bronze, silver, benchmark, custom")
    else:
        print("Uso: python exemplo.py [basico|completo|bronze|silver|benchmark|custom]")
        print("\nExecutando exemplo b√°sico...")
        exemplo_basico()